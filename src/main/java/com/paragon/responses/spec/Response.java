package com.paragon.responses.spec;

import com.fasterxml.jackson.annotation.JsonAutoDetect;
import com.fasterxml.jackson.annotation.JsonCreator;
import com.fasterxml.jackson.annotation.JsonIgnoreProperties;
import com.fasterxml.jackson.annotation.JsonInclude;
import com.fasterxml.jackson.annotation.JsonProperty;
import com.fasterxml.jackson.core.JsonProcessingException;
import com.fasterxml.jackson.databind.ObjectMapper;
import java.util.List;
import java.util.Map;
import java.util.Objects;
import org.jspecify.annotations.NonNull;
import org.jspecify.annotations.Nullable;

/** */
@JsonInclude(JsonInclude.Include.NON_NULL)
@JsonAutoDetect(fieldVisibility = JsonAutoDetect.Visibility.ANY)
@JsonIgnoreProperties(ignoreUnknown = true)
public class Response {
  protected final @Nullable Boolean background;
  protected final @Nullable Conversation conversation;
  protected final @Nullable Number createdAt;
  protected final @Nullable ResponseError error;
  protected final @Nullable String id;
  protected final @Nullable IncompleteDetails incompleteDetails;
  protected final @Nullable ResponseInputItem instructions;
  protected final @Nullable Integer maxOutputTokens;
  protected final @Nullable Integer maxToolCalls;
  protected final @Nullable Map<String, String> metadata;
  protected final @Nullable String model;
  protected final @Nullable ResponseObject object;
  protected final @Nullable List<ResponseOutput> output;
  protected final @Nullable Boolean parallelToolCalls;
  protected final @Nullable PromptTemplate prompt;
  protected final @Nullable String promptCacheKey;
  protected final @Nullable String promptCacheRetention;
  protected final @Nullable Reasoning reasoning;
  protected final @Nullable String safetyIdentifier;
  protected final @Nullable String serviceTier;
  protected final @Nullable ResponseGenerationStatus status;
  protected final @Nullable Number temperature;
  protected final @Nullable TextConfigurationOptions text;
  protected final @Nullable ToolChoice toolChoice;
  protected final @Nullable List<Tool> tools;
  protected final @Nullable Integer topLogprobs;
  protected final @Nullable Number topP;
  protected final @Nullable Truncation truncation;

  /**
   * @param background Whether to run the model response in the background. <a
   *     href="https://platform.openai.com/docs/guides/background">Learn more</a>.
   * @param conversation The conversation that this response belongs to. Input items and output
   *     items from this response are automatically added to this conversation.
   * @param createdAt Unix timestamp (in seconds) of when this Response was created.
   * @param error An error object returned when the model fails to generate a Response.
   * @param id Unique identifier for this Response.
   * @param incompleteDetails Details about why the response is incomplete.
   * @param instructions A system (or developer) message inserted into the model's context.
   *     <p>When using along with previous_response_id, the instructions from a previous response
   *     will not be carried over to the next response. This makes it simple to swap out system (or
   *     developer) messages in new responses.
   * @param maxOutputTokens An upper bound for the number of tokens that can be generated for a
   *     response, including visible output tokens and <a
   *     href="https://platform.openai.com/docs/guides/reasoning">reasoning tokens</a>.
   * @param maxToolCalls The maximum number of total calls to built-in tools that can be processed
   *     in a response. This maximum number applies across all built-in tool calls, not per
   *     individual tool. Any further attempts to call a tool by the model will be ignored.
   * @param metadata Set of 16 key-value pairs that can be attached to an object. This can be useful
   *     for storing additional information about the object in a structured format, and querying
   *     for objects via API or the dashboard.
   *     <p>Keys are strings with a maximum length of 64 characters. Values are strings with a
   *     maximum length of 512 characters.
   * @param model Model ID used to generate the response, like gpt-4o or o3. OpenAI offers a wide
   *     range of models with different capabilities, performance characteristics, and price points.
   *     Refer to the model guide to browse and compare available models.
   * @param object The object type of this resource - always set to {@link ResponseObject}.
   * @param output An array of content items generated by the model.
   *     <p>The length and order of items in the output array is dependent on the model's response.
   *     Rather than accessing the first item in the output array and assuming it's an assistant
   *     message with the content generated by the model, you might consider using the output_text
   *     property where supported in SDKs.
   * @param parallelToolCalls Whether to allow the model to run tool calls in parallel.
   * @param prompt Reference to a prompt template and its variables. <a
   *     href="https://platform.openai
   *     .com/docs/guides/text?api-mode=responses#reusable-prompts">Learn more</a>.
   * @param promptCacheKey Used by OpenAI to cache responses for similar requests to optimize your
   *     cache hit rates. Replaces the user field. <a
   *     href="https://platform.openai.com/docs/guides/prompt-caching">Learn more</a>.
   * @param promptCacheRetention The retention policy for the prompt cache. Set to 24h to enable
   *     extended prompt caching, which keeps cached prefixes active for longer, up to a maximum of
   *     24 hours. Learn more.
   * @param reasoning gpt-5 and o-series models only
   *     <p>Configuration options for <a
   *     href="https://platform.openai.com/docs/guides/reasoning">reasoning models</a>.
   * @param safetyIdentifier A stable identifier used to help detect users of your application that
   *     may be violating OpenAI's usage policies. The IDs should be a string that uniquely
   *     identifies each user. We recommend hashing their username or email address, in order to
   *     avoid sending us any identifying information. <a href="https://platform.openai
   *     .com/docs/guides/safety-best-practices#safety-identifiers">Learn more</a>.
   * @param serviceTier Specifies the processing type used for serving the request.
   *     <p>If set to 'auto', then the request will be processed with the service tier configured in
   *     the Project settings. Unless otherwise configured, the Project will use 'default'. If set
   *     to 'default', then the request will be processed with the standard pricing and performance
   *     for the selected model. If set to 'flex' or 'priority', then the request will be processed
   *     with the corresponding service tier. When not set, the default behavior is 'auto'. When the
   *     service_tier parameter is set, the response body will include the service_tier value based
   *     on the processing mode actually used to serve the request. This response value may be
   *     different from the value set in the parameter.
   * @param status The status of the response generation. One of {@code completed}, {@code failed},
   *     {@code in_progress}, {@code cancelled}, {@code queued}, or {@code incomplete}.
   * @param temperature What sampling temperature to use, between 0 and 2. Higher values like 0.8
   *     will make the output more random, while lower values like 0.2 will make it more focused and
   *     deterministic. We generally recommend altering this or {@code top_p} but not both.
   * @param text Configuration options for a text response from the model. Can be plain text or
   *     structured JSON data. Learn more:
   *     <ul>
   *       <li><a href="https://platform.openai.com/docs/guides/text">Text inputs and outputs</a>
   *       <li><a href="https://platform.openai.com/docs/guides/structured-outputs">Structured
   *           Outputs</a>
   *     </ul>
   *
   * @param toolChoice How the model should select which tool (or tools) to use when generating a
   *     response. See the {@code tools} parameter to see how to specify which tools the model can
   *     call.
   * @param tools An array of tools the model may call while generating a response. You can specify
   *     which tool to use by setting the tool_choice parameter.
   *     <p>We support the following categories of tools:
   *     <p>Built-in tools: Tools that are provided by OpenAI that extend the model's capabilities,
   *     like <a
   *     href="https://platform.openai.com/docs/guides/tools-web-search?api-mode=responses">web
   *     search</a> or <a href="https://platform.openai.com/docs/guides/tools-file-search">file
   *     search</a>. Learn more about <a
   *     href="https://platform.openai.com/docs/guides/tools">built-in tools</a>. MCP Tools:
   *     Integrations with third-party systems via custom MCP servers or predefined connectors such
   *     as Google Drive and SharePoint. Learn more about <a
   *     href="https://platform.openai.com/docs/guides/tools-connectors-mcp">MCP Tools</a>. Function
   *     calls (custom tools): Functions that are defined by you, enabling the model to call your
   *     own code with strongly typed arguments and outputs. Learn more about <a
   *     href="https://platform.openai.com/docs/guides/function-calling">function calling</a>. You
   *     can also use custom tools to call your own code.
   * @param topLogprobs An integer between 0 and 20 specifying the number of most likely tokens to
   *     return at each token position, each with an associated log probability.
   * @param topP An alternative to sampling with temperature, called nucleus sampling, where the
   *     model considers the results of the tokens with top_p probability mass. So 0.1 means only
   *     the tokens comprising the top 10% probability mass are considered.
   *     <p>We generally recommend altering this or {@code temperature} but not both.
   * @param truncation The truncation strategy to use for the model response.
   *     <p>
   *     <ul>
   *       <li>{@code auto}: If the input to this Response exceeds the model's context window size,
   *           the model will truncate the response to fit the context window by dropping items from
   *           the beginning of the conversation.
   *       <li>{@code disabled} (default): If the input size will exceed the context window size for
   *           a model, the request will fail with a 400 error.
   *     </ul>
   */
  @JsonCreator
  public Response(
      @JsonProperty("background") @Nullable Boolean background,
      @JsonProperty("conversation") @Nullable Conversation conversation,
      @JsonProperty("created_at") @Nullable Number createdAt,
      @JsonProperty("error") @Nullable ResponseError error,
      @JsonProperty("id") @Nullable String id,
      @JsonProperty("incomplete_details") @Nullable IncompleteDetails incompleteDetails,
      @JsonProperty("instructions") @Nullable ResponseInputItem instructions,
      @JsonProperty("max_output_tokens") @Nullable Integer maxOutputTokens,
      @JsonProperty("max_tool_calls") @Nullable Integer maxToolCalls,
      @JsonProperty("metadata") @Nullable Map<String, String> metadata,
      @JsonProperty("model") @Nullable String model,
      @JsonProperty("object") @Nullable ResponseObject object,
      @JsonProperty("output") @Nullable List<ResponseOutput> output,
      @JsonProperty("parallel_tool_calls") @Nullable Boolean parallelToolCalls,
      @JsonProperty("prompt") @Nullable PromptTemplate prompt,
      @JsonProperty("prompt_cache_key") @Nullable String promptCacheKey,
      @JsonProperty("prompt_cache_retention") @Nullable String promptCacheRetention,
      @JsonProperty("reasoning") @Nullable Reasoning reasoning,
      @JsonProperty("safety_identifier") @Nullable String safetyIdentifier,
      @JsonProperty("service_tier") @Nullable String serviceTier,
      @JsonProperty("status") @Nullable ResponseGenerationStatus status,
      @JsonProperty("temperature") @Nullable Number temperature,
      @JsonProperty("text") @Nullable TextConfigurationOptions text,
      @JsonProperty("tool_choice") @Nullable ToolChoice toolChoice,
      @JsonProperty("tools") @Nullable List<Tool> tools,
      @JsonProperty("top_logprobs") @Nullable Integer topLogprobs,
      @JsonProperty("top_p") @Nullable Number topP,
      @JsonProperty("truncation") @Nullable Truncation truncation) {
    this.background = background;
    this.conversation = conversation;
    this.createdAt = createdAt;
    this.error = error;
    this.id = id;
    this.incompleteDetails = incompleteDetails;
    this.instructions = instructions;
    this.maxOutputTokens = maxOutputTokens;
    this.maxToolCalls = maxToolCalls;
    this.metadata = metadata;
    this.model = model;
    this.object = object;
    this.output = output;
    this.parallelToolCalls = parallelToolCalls;
    this.prompt = prompt;
    this.promptCacheKey = promptCacheKey;
    this.promptCacheRetention = promptCacheRetention;
    this.reasoning = reasoning;
    this.safetyIdentifier = safetyIdentifier;
    this.serviceTier = serviceTier;
    this.status = status;
    this.temperature = temperature;
    this.text = text;
    this.toolChoice = toolChoice;
    this.tools = tools;
    this.topLogprobs = topLogprobs;
    this.topP = topP;
    this.truncation = truncation;
  }

  // ===== Getters for DTO/Value Object fields =====
  // These are appropriate because Response is an immutable data carrier from the API

  private static boolean numbersEqual(Number a, Number b) {
    if (Objects.equals(a, b)) return true;
    if (a == null || b == null) return false;
    // Compare by double value to handle different Number subclasses
    return Double.compare(a.doubleValue(), b.doubleValue()) == 0;
  }

  public @Nullable Boolean background() {
    return background;
  }

  public @Nullable Conversation conversation() {
    return conversation;
  }

  public @Nullable Number createdAt() {
    return createdAt;
  }

  public @Nullable ResponseError error() {
    return error;
  }

  public @Nullable String id() {
    return id;
  }

  public @Nullable IncompleteDetails incompleteDetails() {
    return incompleteDetails;
  }

  public @Nullable ResponseInputItem instructions() {
    return instructions;
  }

  public @Nullable Integer maxOutputTokens() {
    return maxOutputTokens;
  }

  public @Nullable Integer maxToolCalls() {
    return maxToolCalls;
  }

  public @Nullable Map<String, String> metadata() {
    return metadata;
  }

  public @Nullable String model() {
    return model;
  }

  public @Nullable ResponseObject object() {
    return object;
  }

  public @Nullable List<ResponseOutput> output() {
    return output;
  }

  public @Nullable Boolean parallelToolCalls() {
    return parallelToolCalls;
  }

  public @Nullable PromptTemplate prompt() {
    return prompt;
  }

  public @Nullable String promptCacheKey() {
    return promptCacheKey;
  }

  public @Nullable String promptCacheRetention() {
    return promptCacheRetention;
  }

  public @Nullable Reasoning reasoning() {
    return reasoning;
  }

  public @Nullable String safetyIdentifier() {
    return safetyIdentifier;
  }

  public @Nullable String serviceTier() {
    return serviceTier;
  }

  public @Nullable ResponseGenerationStatus status() {
    return status;
  }

  public @Nullable Number temperature() {
    return temperature;
  }

  public @Nullable TextConfigurationOptions text() {
    return text;
  }

  public @Nullable ToolChoice toolChoice() {
    return toolChoice;
  }

  public @Nullable List<Tool> tools() {
    return tools;
  }

  public @Nullable Integer topLogprobs() {
    return topLogprobs;
  }

  public @Nullable Number topP() {
    return topP;
  }

  public @Nullable Truncation truncation() {
    return truncation;
  }

  public <T> @NonNull ParsedResponse<T> parse(
      @NonNull Class<T> textFormat, ObjectMapper objectMapper) throws JsonProcessingException {
    for (ResponseOutput responseOutput : Objects.requireNonNull(output)) {
      if (responseOutput instanceof Message) {
        String outputText = ((Message) responseOutput).outputText();

        return new ParsedResponse<>(
            background,
            conversation,
            createdAt,
            error,
            id,
            incompleteDetails,
            instructions,
            maxOutputTokens,
            maxToolCalls,
            metadata,
            model,
            object,
            output,
            parallelToolCalls,
            prompt,
            promptCacheKey,
            promptCacheRetention,
            reasoning,
            safetyIdentifier,
            serviceTier,
            status,
            temperature,
            text,
            toolChoice,
            tools,
            topLogprobs,
            topP,
            truncation,
            objectMapper.readValue(outputText, textFormat));
      }
    }

    try {
      throw new RuntimeException(
          String.format(
              "The response could not be parsed. Response: %s",
              objectMapper.writeValueAsString(this)));
    } catch (JsonProcessingException e) {
      throw new RuntimeException(e);
    }
  }

  /**
   * SDK-only convenience property that contains the aggregated text output from all output_text
   * items in the output array, if any are present. Supported in the Python and JavaScript SDKs.
   *
   * @return string representation of the output
   */
  public @Nullable String outputText() {
    if (output == null) {
      return null;
    }
    if (output.isEmpty()) {
      return "";
    }

    var messages =
        output.stream()
            .filter(output -> output instanceof OutputMessage)
            .map(OutputMessage.class::cast)
            .toList();

    StringBuilder stringBuilder = new StringBuilder();
    messages.forEach(stringBuilder::append);
    return stringBuilder.toString();
  }

  public Boolean failed() {
    assert status != null;
    return status.equals(ResponseGenerationStatus.FAILED);
  }

  public @NonNull List<ImageGenerationCall> images() {
    if (output == null) {
      return List.of();
    }

    return output.stream()
        .filter(ImageGenerationCall.class::isInstance)
        .map(ImageGenerationCall.class::cast)
        .toList();
  }

  public @NonNull List<ToolCall> toolCalls() {
    if (output == null) {
      return List.of();
    }

    return output.stream()
        .filter(el -> ToolCall.class.isAssignableFrom(el.getClass()))
        .map(ToolCall.class::cast)
        .toList();
  }

  /**
   * Returns all function tool calls from the response output.
   *
   * <p>Note: These are raw tool calls that cannot be invoked directly.
   *
   * @return list of function tool calls
   */
  public @NonNull List<FunctionToolCall> functionToolCalls() {
    if (output == null) {
      return List.of();
    }

    return output.stream()
        .filter(FunctionToolCall.class::isInstance)
        .map(FunctionToolCall.class::cast)
        .toList();
  }

  /**
   * Returns all function tool calls bound to their implementations from the store.
   *
   * <p>The returned {@link BoundedFunctionCall} instances can be invoked via {@link
   * BoundedFunctionCall#call()}.
   *
   * @param store the store containing the function implementations
   * @return list of callable bounded function calls
   * @throws IllegalArgumentException if any tool call references a function not in the store
   */
  public @NonNull List<BoundedFunctionCall> functionToolCalls(@NonNull FunctionToolStore store) {
    return store.bindAll(functionToolCalls());
  }

  public boolean isIncomplete() {
    return incompleteDetails != null
        && incompleteDetails.reason() != null
        && incompleteDetails.reason().isEmpty();
  }

  @Override
  public boolean equals(Object obj) {
    if (obj == this) return true;
    if (!(obj instanceof Response that)) return false;
    return Objects.equals(this.background, that.background)
        && Objects.equals(this.conversation, that.conversation)
        && numbersEqual(this.createdAt, that.createdAt)
        && Objects.equals(this.error, that.error)
        && Objects.equals(this.id, that.id)
        && Objects.equals(this.incompleteDetails, that.incompleteDetails)
        && Objects.equals(this.instructions, that.instructions)
        && Objects.equals(this.maxOutputTokens, that.maxOutputTokens)
        && Objects.equals(this.maxToolCalls, that.maxToolCalls)
        && Objects.equals(this.metadata, that.metadata)
        && Objects.equals(this.model, that.model)
        && Objects.equals(this.object, that.object)
        && Objects.equals(this.output, that.output)
        && Objects.equals(this.parallelToolCalls, that.parallelToolCalls)
        && Objects.equals(this.prompt, that.prompt)
        && Objects.equals(this.promptCacheKey, that.promptCacheKey)
        && Objects.equals(this.promptCacheRetention, that.promptCacheRetention)
        && Objects.equals(this.reasoning, that.reasoning)
        && Objects.equals(this.safetyIdentifier, that.safetyIdentifier)
        && Objects.equals(this.serviceTier, that.serviceTier)
        && Objects.equals(this.status, that.status)
        && numbersEqual(this.temperature, that.temperature)
        && Objects.equals(this.text, that.text)
        && Objects.equals(this.toolChoice, that.toolChoice)
        && Objects.equals(this.tools, that.tools)
        && Objects.equals(this.topLogprobs, that.topLogprobs)
        && numbersEqual(this.topP, that.topP)
        && Objects.equals(this.truncation, that.truncation);
  }

  @Override
  public int hashCode() {
    return Objects.hash(
        background,
        conversation,
        createdAt,
        error,
        id,
        incompleteDetails,
        instructions,
        maxOutputTokens,
        maxToolCalls,
        metadata,
        model,
        object,
        output,
        parallelToolCalls,
        prompt,
        promptCacheKey,
        promptCacheRetention,
        reasoning,
        safetyIdentifier,
        serviceTier,
        status,
        temperature,
        text,
        toolChoice,
        tools,
        topLogprobs,
        topP,
        truncation);
  }

  @Override
  public @NonNull String toString() {
    return "Response["
        + "background="
        + background
        + ", "
        + "conversation="
        + conversation
        + ", "
        + "createdAt="
        + createdAt
        + ", "
        + "error="
        + error
        + ", "
        + "id="
        + id
        + ", "
        + "incompleteDetails="
        + incompleteDetails
        + ", "
        + "instructions="
        + instructions
        + ", "
        + "maxOutputTokens="
        + maxOutputTokens
        + ", "
        + "maxToolCalls="
        + maxToolCalls
        + ", "
        + "metadata="
        + metadata
        + ", "
        + "model="
        + model
        + ", "
        + "object="
        + object
        + ", "
        + "output="
        + output
        + ", "
        + "parallelToolCalls="
        + parallelToolCalls
        + ", "
        + "prompt="
        + prompt
        + ", "
        + "promptCacheKey="
        + promptCacheKey
        + ", "
        + "promptCacheRetention="
        + promptCacheRetention
        + ", "
        + "reasoning="
        + reasoning
        + ", "
        + "safetyIdentifier="
        + safetyIdentifier
        + ", "
        + "serviceTier="
        + serviceTier
        + ", "
        + "status="
        + status
        + ", "
        + "temperature="
        + temperature
        + ", "
        + "text="
        + text
        + ", "
        + "toolChoice="
        + toolChoice
        + ", "
        + "tools="
        + tools
        + ", "
        + "topLogprobs="
        + topLogprobs
        + ", "
        + "topP="
        + topP
        + ", "
        + "truncation="
        + truncation
        + ']';
  }
}
