package com.paragon.responses.spec;

import java.util.List;
import java.util.Map;
import org.jspecify.annotations.NonNull;
import org.jspecify.annotations.Nullable;

public class ParsedResponse<T> extends Response {
  private final T outputParsed;

  /**
   * @param background Whether to run the model response in the background. <a
   *     href="https://platform.openai.com/docs/guides/background">Learn more</a>.
   * @param conversation The conversation that this response belongs to. Input items and output
   *     items from this response are automatically added to this conversation.
   * @param createdAt Unix timestamp (in seconds) of when this Response was created.
   * @param error An error object returned when the model fails to generate a Response.
   * @param id Unique identifier for this Response.
   * @param incompleteDetails Details about why the response is incomplete.
   * @param instructions A system (or developer) message inserted into the model's context.
   *     <p>When using along with previous_response_id, the instructions from a previous response
   *     will not be carried over to the next response. This makes it simple to swap out system (or
   *     developer) messages in new responses.
   * @param maxOutputTokens An upper bound for the number of tokens that can be generated for a
   *     response, including visible output tokens and <a
   *     href="https://platform.openai.com/docs/guides/reasoning">reasoning tokens</a>.
   * @param maxToolCalls The maximum number of total calls to built-in tools that can be processed
   *     in a response. This maximum number applies across all built-in tool calls, not per
   *     individual tool. Any further attempts to call a tool by the model will be ignored.
   * @param metadata Set of 16 key-value pairs that can be attached to an object. This can be useful
   *     for storing additional information about the object in a structured format, and querying
   *     for objects via API or the dashboard.
   *     <p>Keys are strings with a maximum length of 64 characters. Values are strings with a
   *     maximum length of 512 characters.
   * @param model Model ID used to generate the response, like gpt-4o or o3. OpenAI offers a wide
   *     range of models with different capabilities, performance characteristics, and price points.
   *     Refer to the model guide to browse and compare available models.
   * @param object The object type of this resource - always set to {@link ResponseObject}.
   * @param output An array of content items generated by the model.
   *     <p>The length and order of items in the output array is dependent on the model's response.
   *     Rather than accessing the first item in the output array and assuming it's an assistant
   *     message with the content generated by the model, you might consider using the output_text
   *     property where supported in SDKs.
   * @param parallelToolCalls Whether to allow the model to run tool calls in parallel.
   * @param prompt Reference to a prompt template and its variables. <a
   *     href="https://platform.openai
   *     .com/docs/guides/text?api-mode=responses#reusable-prompts">Learn more</a>.
   * @param promptCacheKey Used by OpenAI to cache responses for similar requests to optimize your
   *     cache hit rates. Replaces the user field. <a
   *     href="https://platform.openai.com/docs/guides/prompt-caching">Learn more</a>.
   * @param promptCacheRetention The retention policy for the prompt cache. Set to 24h to enable
   *     extended prompt caching, which keeps cached prefixes active for longer, up to a maximum of
   *     24 hours. Learn more.
   * @param reasoning gpt-5 and o-series models only
   *     <p>Configuration options for <a
   *     href="https://platform.openai.com/docs/guides/reasoning">reasoning models</a>.
   * @param safetyIdentifier A stable identifier used to help detect users of your application that
   *     may be violating OpenAI's usage policies. The IDs should be a string that uniquely
   *     identifies each user. We recommend hashing their username or email address, in order to
   *     avoid sending us any identifying information. <a href="https://platform.openai
   *     .com/docs/guides/safety-best-practices#safety-identifiers">Learn more</a>.
   * @param serviceTier Specifies the processing type used for serving the request.
   *     <p>If set to 'auto', then the request will be processed with the service tier configured in
   *     the Project settings. Unless otherwise configured, the Project will use 'default'. If set
   *     to 'default', then the request will be processed with the standard pricing and performance
   *     for the selected model. If set to 'flex' or 'priority', then the request will be processed
   *     with the corresponding service tier. When not set, the default behavior is 'auto'. When the
   *     service_tier parameter is set, the response body will include the service_tier value based
   *     on the processing mode actually used to serve the request. This response value may be
   *     different from the value set in the parameter.
   * @param status The status of the response generation. One of {@code completed}, {@code failed},
   *     {@code in_progress}, {@code cancelled}, {@code queued}, or {@code incomplete}.
   * @param temperature What sampling temperature to use, between 0 and 2. Higher values like 0.8
   *     will make the output more random, while lower values like 0.2 will make it more focused and
   *     deterministic. We generally recommend altering this or {@code top_p} but not both.
   * @param text Configuration options for a text response from the model. Can be plain text or
   *     structured JSON data. Learn more:
   *     <ul>
   *       <li><a href="https://platform.openai.com/docs/guides/text">Text inputs and outputs</a>
   *       <li><a href="https://platform.openai.com/docs/guides/structured-outputs">Structured
   *           Outputs</a>
   *     </ul>
   *
   * @param toolChoice How the model should select which tool (or tools) to use when generating a
   *     response. See the {@code tools} parameter to see how to specify which tools the model can
   *     call.
   * @param tools An array of tools the model may call while generating a response. You can specify
   *     which tool to use by setting the tool_choice parameter.
   *     <p>We support the following categories of tools:
   *     <p>Built-in tools: Tools that are provided by OpenAI that extend the model's capabilities,
   *     like <a
   *     href="https://platform.openai.com/docs/guides/tools-web-search?api-mode=responses">web
   *     search</a> or <a href="https://platform.openai.com/docs/guides/tools-file-search">file
   *     search</a>. Learn more about <a
   *     href="https://platform.openai.com/docs/guides/tools">built-in tools</a>. MCP Tools:
   *     Integrations with third-party systems via custom MCP servers or predefined connectors such
   *     as Google Drive and SharePoint. Learn more about <a
   *     href="https://platform.openai.com/docs/guides/tools-connectors-mcp">MCP Tools</a>. Function
   *     calls (custom tools): Functions that are defined by you, enabling the model to call your
   *     own code with strongly typed arguments and outputs. Learn more about <a
   *     href="https://platform.openai.com/docs/guides/function-calling">function calling</a>. You
   *     can also use custom tools to call your own code.
   * @param topLogprobs An integer between 0 and 20 specifying the number of most likely tokens to
   *     return at each token position, each with an associated log probability.
   * @param topP An alternative to sampling with temperature, called nucleus sampling, where the
   *     model considers the results of the tokens with top_p probability mass. So 0.1 means only
   *     the tokens comprising the top 10% probability mass are considered.
   *     <p>We generally recommend altering this or {@code temperature} but not both.
   * @param truncation The truncation strategy to use for the model response.
   *     <p>
   *     <ul>
   *       <li>{@code auto}: If the input to this Response exceeds the model's context window size,
   *           the model will truncate the response to fit the context window by dropping items from
   *           the beginning of the conversation.
   *       <li>{@code disabled} (default): If the input size will exceed the context window size for
   *           a model, the request will fail with a 400 error.
   *     </ul>
   */
  public ParsedResponse(
      @Nullable Boolean background,
      @Nullable Conversation conversation,
      @Nullable Number createdAt,
      @Nullable ResponseError error,
      @Nullable String id,
      @Nullable IncompleteDetails incompleteDetails,
      @Nullable ResponseInputItem instructions,
      @Nullable Integer maxOutputTokens,
      @Nullable Integer maxToolCalls,
      @Nullable Map<String, String> metadata,
      @Nullable String model,
      @Nullable ResponseObject object,
      @Nullable List<ResponseOutput> output,
      @Nullable Boolean parallelToolCalls,
      @Nullable PromptTemplate prompt,
      @Nullable String promptCacheKey,
      @Nullable String promptCacheRetention,
      @Nullable Reasoning reasoning,
      @Nullable String safetyIdentifier,
      @Nullable String serviceTier,
      @Nullable ResponseGenerationStatus status,
      @Nullable Number temperature,
      @Nullable TextConfigurationOptions text,
      @Nullable ToolChoice toolChoice,
      @Nullable List<Tool> tools,
      @Nullable Integer topLogprobs,
      @Nullable Number topP,
      @Nullable Truncation truncation,
      @NonNull T outputParsed) {
    super(
        background,
        conversation,
        createdAt,
        error,
        id,
        incompleteDetails,
        instructions,
        maxOutputTokens,
        maxToolCalls,
        metadata,
        model,
        object,
        output,
        parallelToolCalls,
        prompt,
        promptCacheKey,
        promptCacheRetention,
        reasoning,
        safetyIdentifier,
        serviceTier,
        status,
        temperature,
        text,
        toolChoice,
        tools,
        topLogprobs,
        topP,
        truncation);
    this.outputParsed = outputParsed;
  }

  public @NonNull T outputParsed() {
    return outputParsed;
  }
}
